{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theoritical Questions**"
      ],
      "metadata": {
        "id": "Jk06i0mmqTWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is statistics, and why is it important?\n",
        "* Statistics is the science of collecting, organizing, analyzing, and interpreting data. It helps us make informed decisions by identifying trends, testing hypotheses, and drawing conclusions from data. Statistics is crucial in various fields such as medicine, business, social sciences, and government, where decisions often rely on data analysis. For instance, companies use statistics to understand consumer behavior, while governments use it to plan policies. It helps reduce uncertainty and provides a way to make data-driven decisions in the presence of variability or incomplete information.\n",
        "2. 2. What are the two main types of statistics?\n",
        "* The two main types of statistics are descriptive and inferential statistics. Descriptive statistics involves methods for summarizing and organizing data using measures like mean, median, mode, and standard deviation. It helps present raw data in a meaningful way. Inferential statistics, on the other hand, uses sample data to make generalizations or predictions about a population. It involves hypothesis testing, confidence intervals, and regression analysis. Together, these two types help in understanding data comprehensively and making predictions or decisions based on limited observations.\n",
        "3. What are descriptive statistics?\n",
        "* Descriptive statistics refer to methods used to summarize or describe the main features of a dataset. This includes measures of central tendency (mean, median, mode), measures of dispersion (range, variance, standard deviation), and graphical representations (bar graphs, histograms, pie charts). Descriptive statistics help to simplify large amounts of data in a sensible way, making it easier to understand patterns, trends, and outliers. For example, a company might use descriptive statistics to summarize sales data for a month, showing the average sales per day and the variability across days.\n",
        "3. What are descriptive statistics?\n",
        "* Descriptive statistics refer to methods used to summarize or describe the main features of a dataset. This includes measures of central tendency (mean, median, mode), measures of dispersion (range, variance, standard deviation), and graphical representations (bar graphs, histograms, pie charts). Descriptive statistics help to simplify large amounts of data in a sensible way, making it easier to understand patterns, trends, and outliers. For example, a company might use descriptive statistics to summarize sales data for a month, showing the average sales per day and the variability across days.\n",
        "4. What is inferential statistics?\n",
        "* Inferential statistics involves using a sample of data to make inferences or predictions about a larger population. It allows researchers to test hypotheses and estimate population parameters using tools like t-tests, chi-square tests, and regression analysis. For example, if a political poll surveys 1,000 voters, inferential statistics helps predict the preferences of the entire voter population. The goal is to make reliable conclusions while accounting for the uncertainty inherent in working with samples rather than full populations.\n",
        "5. What is sampling in statistics?\n",
        "* Sampling in statistics is the process of selecting a subset of individuals or observations from a larger population to represent the whole. Since studying an entire population is often impractical, sampling makes research more manageable and cost-effective. A good sample should be representative to allow accurate generalizations. For example, a health study may examine the dietary habits of 500 people to draw conclusions about an entire city. The quality of sampling directly affects the validity and reliability of the statistical conclusions.\n",
        "6. What are the different types of sampling methods?\n",
        "* There are two broad categories of sampling methods: probability sampling and non-probability sampling. Probability methods include simple random sampling, stratified sampling, systematic sampling, and cluster sampling—each giving every member of the population a known chance of being selected. Non-probability methods include convenience sampling, judgmental sampling, quota sampling, and snowball sampling, where selection is based on subjective judgment or accessibility. The choice of method depends on research goals, resources, and the need for representativeness and accuracy.\n",
        "7. What is the difference between random and non-random sampling?\n",
        "* Random sampling involves selecting samples in such a way that every member of the population has an equal chance of being chosen. It reduces bias and ensures the sample is representative of the population. Non-random sampling, on the other hand, does not give all individuals equal chances of selection. It includes methods like convenience and judgmental sampling and may introduce bias. For example, surveying only people at a mall (non-random) might not represent an entire city, while randomly selecting individuals from a city registry (random) offers more reliable results.\n",
        "8. Define and give examples of qualitative and quantitative data.\n",
        "* Qualitative data is descriptive and represents characteristics that cannot be measured numerically, such as color, gender, or brand preference (e.g., \"blue shirt,\" \"female\"). Quantitative data is numerical and can be measured, such as height, age, or income (e.g., \"180 cm,\" \"25 years,\" \"$50,000\"). Quantitative data can further be classified as discrete (e.g., number of students) or continuous (e.g., weight). Qualitative data is often used for categorizing or labeling, while quantitative data is used for statistical calculations and numerical analysis.\n",
        "9. What are the different types of data in statistics?\n",
        "* Data in statistics can be classified into four main types: nominal, ordinal, interval, and ratio. Nominal data categorizes without order (e.g., gender, color). Ordinal data has a ranked order but unequal differences (e.g., satisfaction levels). Interval data has equal intervals but no true zero (e.g., temperature in Celsius). Ratio data has both equal intervals and a true zero (e.g., height, weight, age). Understanding data types is crucial for choosing the appropriate statistical methods for analysis and interpretation.\n",
        "10. Explain nominal, ordinal, interval, and ratio levels of measurement.\n",
        "* Nominal level includes data that can be categorized but not ordered (e.g., hair color, nationality). Ordinal level data involves categories with a meaningful order but unknown interval distances (e.g., education level: high school, college, graduate). Interval level has ordered values with known and equal distances between them, but no absolute zero (e.g., IQ scores, temperature in Celsius). Ratio level data has all the features of interval data plus a meaningful zero, allowing for the calculation of ratios (e.g., weight, income). Each level determines the kind of statistical analysis that can be performed.\n",
        "11. What is the measure of central tendency?\n",
        "* The measure of central tendency refers to statistical measures that describe the center point or typical value of a dataset. The three main measures are the mean (average), median (middle value), and mode (most frequent value). These measures help summarize a large dataset with a single representative value. Choosing the right measure depends on the type of data and its distribution. For example, the median is preferred over the mean when the data has outliers or is skewed.\n",
        "12. Define mean, median, and mode.\n",
        "* The mean is the arithmetic average of all data values, calculated by summing them and dividing by the number of observations. The median is the middle value when the data is arranged in ascending order; if there's an even number of observations, it's the average of the two middle values. The mode is the value that appears most frequently in the dataset. For example, in the set {2, 4, 4, 5}, the mean is 3.75, the median is 4, and the mode is 4.\n",
        "13. What is the significance of the measure of central tendency?\n",
        "* Measures of central tendency provide a quick summary of a dataset by identifying its central or typical value. They help in understanding the general trend, making comparisons, and facilitating decision-making in various fields like business, education, and healthcare. For instance, knowing the average test score helps teachers assess overall class performance. They also help detect outliers and understand data distribution. Each measure—mean, median, and mode—offers a different perspective, and choosing the appropriate one depends on the nature of the data and its distribution.\n",
        "14. What is variance, and how is it calculated?\n",
        "* Variance measures the spread or dispersion of a dataset. It shows how far each data point is from the mean. A higher variance indicates that data points are more spread out. To calculate variance, subtract the mean from each data point, square the result, sum all squared values, and divide by the number of observations (for population) or by one less (for sample). For example, in a dataset {2, 4, 6}, the variance would be calculated as the average of the squared deviations from the mean (4).\n",
        "15. What is standard deviation, and why is it important?\n",
        "* Standard deviation is the square root of the variance and provides a measure of how spread out data values are around the mean. Unlike variance, it’s in the same units as the data, making interpretation easier. A low standard deviation indicates that values are close to the mean, while a high one shows more variability. It’s essential in assessing risk and reliability, such as in finance to gauge investment volatility or in quality control to monitor product consistency.\n",
        "16. Define and explain the term range in statistics.\n",
        "* The range in statistics is a measure of dispersion that represents the difference between the highest and lowest values in a dataset. It provides a simple way to understand the spread of the data. For example, if the highest exam score is 95 and the lowest is 65, the range is 30. While easy to calculate, the range is sensitive to outliers and doesn't reflect data distribution well, so it’s often used alongside other measures like variance and standard deviation.\n",
        "17. What is the difference between variance and standard deviation?\n",
        "* Both variance and standard deviation measure the spread of data around the mean. The key difference is that variance is the average of the squared deviations from the mean, while standard deviation is the square root of the variance. Standard deviation is in the same units as the original data, making it more interpretable. For example, if variance in income is 10,000 (dollars squared), the standard deviation would be about 100 (dollars), which is easier to understand in real-world terms.\n",
        "18. What is skewness in a dataset?\n",
        "* Skewness describes the asymmetry of a dataset’s distribution. A perfectly symmetrical distribution has zero skewness. If a dataset has more values on one side of the mean, it becomes skewed. Positive skew means a longer tail on the right (more high values), while negative skew means a longer tail on the left (more low values). Skewness helps in understanding the shape and tendency of data distribution, guiding decisions like which central tendency measure to use or which statistical tests are appropriate.\n",
        "19. What does it mean if a dataset is positively or negatively skewed?\n",
        "* A positively skewed dataset has a long tail on the right side, indicating that most values are concentrated on the left but with some high-value outliers (e.g., income data). The mean is usually greater than the median. A negatively skewed dataset has a long tail on the left, with most values on the higher end but a few low outliers (e.g., test scores where most students did well). In such distributions, the mean is less than the median.\n",
        "20. Define and explain kurtosis.\n",
        "* Kurtosis measures the \"tailedness\" or the sharpness of the peak in a data distribution. It indicates whether data are heavy-tailed or light-tailed relative to a normal distribution. A normal distribution has a kurtosis of 3 (mesokurtic). Leptokurtic distributions have higher peaks and fatter tails (kurtosis > 3), suggesting more outliers. Platykurtic distributions have flatter peaks and thinner tails (kurtosis < 3), suggesting fewer outliers. Understanding kurtosis helps in identifying extreme values and assessing risk in fields like finance and quality control.\n",
        "21. What is the purpose of covariance?\n",
        "* Covariance measures the directional relationship between two variables. A positive covariance means that as one variable increases, the other tends to increase; a negative covariance indicates an inverse relationship. It helps in understanding how variables move together. For instance, in finance, a positive covariance between two stocks suggests they tend to rise or fall together. However, the magnitude of covariance can be hard to interpret due to dependence on units, so correlation is often preferred for standardized comparison.\n",
        "22. What does correlation measure in statistics?\n",
        "* Correlation quantifies the strength and direction of the linear relationship between two variables. It is a standardized measure ranging from -1 to 1. A correlation of +1 means a perfect positive relationship, -1 indicates a perfect negative relationship, and 0 means no linear relationship. For example, if height and weight have a correlation of 0.8, it means that taller people tend to weigh more, with a strong positive association. Correlation helps in prediction, analysis, and understanding interdependencies between variables.\n",
        "23. What is the difference between covariance and correlation?\n",
        "* While both covariance and correlation measure the relationship between two variables, covariance indicates the direction of the relationship and is not standardized, meaning its value depends on the units of measurement. Correlation is a standardized version of covariance, ranging from -1 to +1, making it easier to interpret and compare across datasets. For example, two variables might have a high covariance, but without knowing the scale, it’s unclear how strong the relationship is—correlation solves this by providing a normalized measure.\n",
        "24. What are some real-world applications of statistics?\n",
        "* Statistics is used in nearly every field. In healthcare, it's vital for analyzing clinical trials and predicting disease outbreaks. Businesses use it to forecast sales, analyze market trends, and optimize operations. Governments rely on statistics for policy-making, census data, and economic planning. In sports, teams use statistics to evaluate player performance and game strategies. In education, it helps assess student progress and curriculum effectiveness. Even weather forecasts and quality control in manufacturing depend heavily on statistical analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-H4dtVLVqX8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Questions**"
      ],
      "metadata": {
        "id": "aFYc7un8s4mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. How do you calculate the mean, median, and mode of a dataset?\n",
        "* import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "data = [10, 20, 20, 30, 40, 50, 60]\n",
        "\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "mode = stats.mode(data, keepdims=True)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Median:\", median)\n",
        "print(\"Mode:\", mode.mode[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "sRGxF0V7s-8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Write a Python program to compute the variance and standard deviation of a dataset?\n",
        "* import numpy as np\n",
        "\n",
        "data = [10, 20, 20, 30, 40, 50, 60]\n",
        "\n",
        "variance = np.var(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "print(\"Variance:\", variance)\n",
        "print(\"Standard Deviation:\", std_dev)\n"
      ],
      "metadata": {
        "id": "odT9NBAOtgOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. Create a dataset and classify it into nominal, ordinal, interval, and ratio types?\n",
        "* import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Nominal': ['Red', 'Blue', 'Green'],\n",
        "    'Ordinal': ['Low', 'Medium', 'High'],\n",
        "    'Interval': [10, 20, 30],\n",
        "    'Ratio': [100, 200, 300]\n",
        "})\n",
        "\n",
        "print(data.dtypes)\n"
      ],
      "metadata": {
        "id": "oOj5zHSBtlsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. Implement sampling techniques like random sampling and stratified sampling?\n",
        "* import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Value': [10, 20, 30, 40, 50, 60]\n",
        "})\n",
        "\n",
        "# Random Sampling\n",
        "random_sample = df.sample(n=3)\n",
        "\n",
        "# Stratified Sampling\n",
        "stratified_sample, _ = train_test_split(df, test_size=0.5, stratify=df['Category'])\n",
        "\n",
        "print(\"Random Sample:\\n\", random_sample)\n",
        "print(\"Stratified Sample:\\n\", stratified_sample)\n"
      ],
      "metadata": {
        "id": "dW9L5E_Tt-QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Write a Python function to calculate the range of a dataset.\n",
        "* import numpy as np\n",
        "\n",
        "data = [10, 20, 30, 40, 50]\n",
        "\n",
        "range_value = np.ptp(data)\n",
        "\n",
        "print(\"Range:\", range_value)\n"
      ],
      "metadata": {
        "id": "6sCa5ncUuC5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. Create a dataset and plot its histogram to visualize skewness?\n",
        "* import matplotlib.pyplot as plt\n",
        "\n",
        "data = [10, 20, 20, 30, 40, 50, 100]\n",
        "\n",
        "plt.hist(data, bins=10)\n",
        "plt.title('Histogram')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zTiQ4MQfuNPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7. 5 Calculate skewness and kurtosis of a dataset using Python libraries?\n",
        "* from scipy.stats import skew, kurtosis\n",
        "\n",
        "data = [10, 20, 20, 30, 40, 50, 100]\n",
        "\n",
        "skewness = skew(data)\n",
        "kurt = kurtosis(data)\n",
        "\n",
        "print(\"Skewness:\", skewness)\n",
        "print(\"Kurtosis:\", kurt)\n"
      ],
      "metadata": {
        "id": "CA4_4AZJuUnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Generate a dataset and demonstrate positive and negative skewness?\n",
        "* import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skewnorm\n",
        "\n",
        "# Positive Skew\n",
        "data_pos = skewnorm.rvs(a=10, size=1000)\n",
        "\n",
        "# Negative Skew\n",
        "data_neg = skewnorm.rvs(a=-10, size=1000)\n",
        "\n",
        "plt.hist(data_pos, bins=30, alpha=0.5, label='Positive Skew')\n",
        "plt.hist(data_neg, bins=30, alpha=0.5, label='Negative Skew')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "tUp2DPoluYsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Write a Python script to calculate covariance between two datasets?\n",
        "* import numpy as np\n",
        "\n",
        "x = [10, 20, 30, 40, 50]\n",
        "y = [15, 25, 35, 45, 55]\n",
        "\n",
        "cov_matrix = np.cov(x, y)\n",
        "covariance = cov_matrix[0][1]\n",
        "\n",
        "print(\"Covariance:\", covariance)\n"
      ],
      "metadata": {
        "id": "xn3cDSbtudaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. Write a Python script to calculate the correlation coefficient between two dataset.\n",
        "* import numpy as np\n",
        "\n",
        "x = (10, 20, 30, 40, 50)\n",
        "y = (15, 25, 35, 45, 55)\n",
        "\n",
        "correlation = np.corrcoef(x, y)[0][1]\n",
        "\n",
        "print(\"Correlation Coefficient:\", correlation)\n"
      ],
      "metadata": {
        "id": "CCoqqk9cuh6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11. Create a scatter plot to visualize the relationship between two variables.\n",
        "* import matplotlib.pyplot as plt\n",
        "\n",
        "x = (10, 20, 30, 40, 50)\n",
        "y = (15, 25, 35, 45, 55)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.title('Scatter Plot')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8Quc_TGNunGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12. Implement and compare simple random sampling and systematic sampling.\n",
        "* import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Value': range(1, 101)})\n",
        "\n",
        "# Simple Random Sampling\n",
        "random_sample = df.sample(n=10)\n",
        "\n",
        "# Systematic Sampling\n",
        "systematic_sample = df.iloc[::10]\n",
        "\n",
        "print(\"Random Sample:\\n\", random_sample)\n",
        "print(\"Systematic Sample:\\n\", systematic_sample)\n"
      ],
      "metadata": {
        "id": "5I9RVslPurkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13. Calculate the mean, median, and mode of grouped data.\n",
        "* import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Class Interval': ['0-10', '10-20', '20-30'],\n",
        "    'Frequency': [5, 15, 10]\n",
        "})\n",
        "\n",
        "# Calculate class marks\n",
        "data['Class Mark'] = data['Class Interval'].apply(lambda x: sum(map(int, x.split('-')))/2)\n",
        "\n",
        "# Mean\n",
        "mean = (data['Class Mark'] * data['Frequency']).sum() / data['Frequency'].sum()\n",
        "\n",
        "print(\"Mean of Grouped Data:\", mean)\n"
      ],
      "metadata": {
        "id": "9v-76hz0uziB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14. Simulate data using Python and calculate its central tendency and dispersion.\n",
        "import numpy as np\n",
        "\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "mean = np.mean(data)\n",
        "median = np.median(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Median:\", median)\n",
        "print(\"Standard Deviation:\", std_dev)\n"
      ],
      "metadata": {
        "id": "MnWFyyAvu3x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15.  Use NumPy or pandas to summarize a dataset’s descriptive statistics.\n",
        "* import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.DataFrame({'Values': np.random.rand(100)})\n",
        "\n",
        "summary = data.describe()\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "abYnTdJNu_0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16.  Plot a boxplot to understand the spread and identify outliers.\n",
        "* import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = np.random.rand(100)\n",
        "\n",
        "plt.boxplot(data)\n",
        "plt.title('Boxplot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "juUHqHMZvEul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17. Calculate the interquartile range (IQR) of a dataset.\n",
        "* import numpy as np\n",
        "\n",
        "data = np.random.rand(100)\n",
        "\n",
        "q1 = np.percentile(data, 25)\n",
        "q3 = np.percentile(data, 75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "print(\"Interquartile Range (IQR):\", iqr)\n"
      ],
      "metadata": {
        "id": "hbK8FYRbvIx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "18. Implement Z-score normalization and explain its significance.\n",
        "* import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "data = np.random.rand(100)\n",
        "\n",
        "z_scores = stats.zscore(data)\n",
        "\n",
        "print(\"Z-score Normalized Data:\\n\", z_scores)\n"
      ],
      "metadata": {
        "id": "KPbq5qAEvNEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "19. Compare two datasets using their standard deviations.\n",
        "import numpy as np\n",
        "\n",
        "data1 = np.random.rand(100)\n",
        "data2 = np.random.rand(100)\n",
        "\n",
        "std_dev1 = np.std(data1)\n",
        "std_dev2 = np.std(data2)\n",
        "\n",
        "print(\"Standard Deviation of Dataset 1:\", std_dev1)\n",
        "print(\"Standard Deviation of Dataset 2:\", std_dev2)\n"
      ],
      "metadata": {
        "id": "H2p3CzHavSdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "20. Write a Python program to visualize covariance using a heatmap.\n",
        "* import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.DataFrame(np.random.rand(100, 3), columns=['A', 'B', 'C'])\n",
        "\n",
        "cov_matrix = data.cov()\n",
        "\n",
        "sns.heatmap(cov_matrix, annot=True)\n",
        "plt.title('Covariance Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SuZc6vWbvfrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "21.  Use seaborn to create a correlation matrix for a dataset.\n",
        "* import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.DataFrame(np.random.rand(100, 3), columns=['A', 'B', 'C'])\n",
        "\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cSHfCnubvkKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "22. Generate a dataset and implement both variance and standard deviation computations.\n",
        "* import numpy as np\n",
        "\n",
        "data = np.random.rand(100)\n",
        "\n",
        "variance = np.var(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "print(\"Variance:\", variance)\n",
        "print(\"Standard Deviation:\", std_dev)\n"
      ],
      "metadata": {
        "id": "QxxqdvVrvso7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "23. Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn.\n",
        "* import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "data = np.random.rand(100)\n",
        "\n",
        "sns.histplot(data, kde=True)\n",
        "plt.title('Histogram with KDE')\n",
        "plt.show()\n",
        "\n",
        "print(\"Skewness:\", skew(data))\n",
        "print(\"Kurtosis:\", kurtosis(data))\n"
      ],
      "metadata": {
        "id": "-ksYUI16vxl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "24.  Implement the Pearson and Spearman correlation coefficients for a dataset.\n",
        "* import numpy as np\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "x = np.random.rand(100)\n",
        "y = np.random.rand(100)\n",
        "\n",
        "pearson_corr, _ = pearsonr(x, y)\n",
        "spearman_corr, _ = spearmanr(x, y)\n",
        "\n",
        "print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
        "print(\"Spearman Correlation Coefficient:\", spearman_corr)\n"
      ],
      "metadata": {
        "id": "ihi1a1zxv2DH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}